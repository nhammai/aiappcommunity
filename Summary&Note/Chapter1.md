# Chapter 1

Chapter 1 này tác giả giới thiệu sơ lượt về lịch sử và nguyên lý đằng sau của LLMs ( Large Language Models). 

Về căn bản mình thấy nắm được LLM phụ thuộc rất nhiều về **context**. Context càng tốt thì output đưa ra càng tốt. 

Đó là nguyên tắc chính để chúng ta viết prompt (sẽ viết về prompt ở phần khác) 

Để hiểu chi tiết hơn các bạn có thể tham khảo các link bên dưới: 

# Paper

## A Survey of Large Language Models

Mình rất recommend đọc paper này nhé. Viết rất chi là chi tiết về LLMs 

[https://huggingface.co/papers/2303.18223](https://huggingface.co/papers/2303.18223)

[https://github.com/RUCAIBox/LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)

## Blog

[https://huyenchip.com/2023/08/16/llm-research-open-challenges.html](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html)

# Video:

## ****Attention for Neural Networks, Clearly Explained!!!****

[Attention for Neural Networks, Clearly Explained!!!](https://www.youtube.com/watch?v=PSs6nxngL6k)

## ****Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!****

[Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!](https://youtu.be/zxQyTK8quyY)

## ****The Evolution Of ChatGPT From GPT-1 To GPT-4****

[The Evolution Of ChatGPT From GPT-1 To GPT-4](https://youtu.be/zqSJG39HQoM)

## State of GPT by ****Andrej Karpathy****

[State of GPT | BRK216HFS](https://www.youtube.com/watch?v=bZQun8Y4L2A)